{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.8' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Set Tesseract path\n",
    "pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "\n",
    "    # Try extracting text using pdfplumber (for digital PDFs)\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted = page.extract_text()\n",
    "            if extracted:\n",
    "                text += extracted + \"\\n\"\n",
    "\n",
    "    # If pdfplumber fails, use OCR\n",
    "    if not text.strip():\n",
    "        images = convert_from_path(pdf_path, dpi=300)\n",
    "        for image in images:\n",
    "            text += pytesseract.image_to_string(image, lang=\"eng\") + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# Extract text and store it in a variable for the marking scheme model\n",
    "pdf_path = \"/content/view_marking_scheme_removed.pdf\"\n",
    "teacher_answer = extract_text_from_pdf(pdf_path)\n",
    "# Save extracted text to a .txt file\n",
    "output_txt_path = \"teacher_answer.txt\"\n",
    "with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(teacher_answer)\n",
    "\n",
    "# The extracted text can now be used as input for the marking scheme model\n",
    "print(teacher_answer)  # Check output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student   Answer   OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Google Cloud credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/prefab-builder-452209-f8-ca411802c1fb.json\"\n",
    "\n",
    "# Function to preprocess image\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "    return binary\n",
    "\n",
    "# Function to perform OCR using Google Cloud Vision\n",
    "def ocr_with_google_vision(image_path):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    return texts[0].description if texts else \"\"\n",
    "\n",
    "# Function to extract and structure text\n",
    "def extract_and_structure_text(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    structured_data = {}\n",
    "    full_text = []  # Stores extracted text for all pages\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = f\"page_{i+1}.png\"\n",
    "        image.save(image_path, 'PNG')\n",
    "        preprocessed_image = preprocess_image(cv2.imread(image_path))\n",
    "        text = ocr_with_google_vision(image_path)\n",
    "        full_text.append(text)  # Store text for each page\n",
    "\n",
    "        print(f\"--- Extracted Text from Page {i+1} ---\\n{text}\\n\")\n",
    "\n",
    "        # Extract question numbers, sub-parts, and marks using regex\n",
    "        questions = re.findall(r'(\\d+[a-z]*)\\.\\s*(.*?)\\s*\\((\\d+)\\)', text)\n",
    "        for question in questions:\n",
    "            q_num, q_text, marks = question\n",
    "            if q_num not in structured_data:\n",
    "                structured_data[q_num] = []\n",
    "            structured_data[q_num].append({\"text\": q_text, \"marks\": int(marks)})\n",
    "\n",
    "    return structured_data, full_text\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = \"/content/view_ans_sheet9.pdf\"\n",
    "\n",
    "# Extract and structure text\n",
    "student_answer, full_text = extract_and_structure_text(pdf_path)\n",
    "\n",
    "# Store text starting from page 3 onwards\n",
    "student_answer_page3 = \"\\n\".join(full_text[2:])  # Pages are 0-indexed, so page 3 is index 2\n",
    "\n",
    "# Save extracted text from page 3 onwards to a .txt file\n",
    "output_txt_path = \"student_answer_page3.txt\"\n",
    "with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(student_answer_page3)\n",
    "\n",
    "# Print output (optional)\n",
    "print(\"Structured Data for Student Answer:\", student_answer)\n",
    "# # Save structured data to JSON\n",
    "# with open(\"structured_answers.json\", \"w\") as json_file:\n",
    "#     json.dump(structured_data, json_file, indent=4)\n",
    "\n",
    "# print(\"Structured data saved to structured_answers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marking   Scheme   Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence-BERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Preprocess text by removing stop words\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def sentiment_analysis(teacher_answer, student_answer):\n",
    "    # Load the NLP model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc_teacher = nlp(teacher_answer)\n",
    "    doc_student = nlp(student_answer)\n",
    "\n",
    "\n",
    "    # Using TextBlob for sentiment analysis\n",
    "    blob_teacher_answer = TextBlob(teacher_answer)\n",
    "    blob_student_answer = TextBlob(student_answer)\n",
    "\n",
    "\n",
    "    teacher_answer_polarity = blob_teacher_answer.sentiment.polarity\n",
    "    student_answer_polarity = blob_student_answer.sentiment.polarity\n",
    "\n",
    "\n",
    "    teacher_answer_subjectivity = blob_teacher_answer.sentiment.subjectivity\n",
    "    student_answer_subjectivity = blob_student_answer.sentiment.subjectivity\n",
    "\n",
    "    # Extracting positive and negative sentiment scores\n",
    "    def get_sentiment_scores(polarity):\n",
    "        return max(0, polarity), abs(min(0, polarity))\n",
    "\n",
    "    teacher_answer_positive, teacher_answer_negative = get_sentiment_scores(teacher_answer)\n",
    "    student_answer_positive, student_answer_negative = get_sentiment_scores(student_answer)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"teacher_answer\": {\n",
    "            \"Polarity\": teacher_answer_polarity,\n",
    "            \"Positive Score\": teacher_answer_positive,\n",
    "            \"Negative Score\": teacher_answer_negative,\n",
    "            # \"TextBlob Subjectivity\": teacher_subjectivity,\n",
    "            # \"Tokens\": [token.text for token in doc_teacher],\n",
    "            # \"Lemmatized\": [token.lemma_ for token in doc_teacher],\n",
    "            # \"POS Tags\": [(token.text, token.pos_) for token in doc_teacher]\n",
    "        },\n",
    "        \"student_answer\": {\n",
    "            \"Polarity\": student_answer_polarity,\n",
    "            \"Positive Score\": student_answer_positive,\n",
    "            \"Negative Score\": student_answer_negative,\n",
    "            # \"TextBlob Subjectivity\": student_100_subjectivity,\n",
    "            # \"Tokens\": [token.text for token in doc_student_100],\n",
    "            # \"Lemmatized\": [token.lemma_ for token in doc_student_100],\n",
    "            # \"POS Tags\": [(token.text, token.pos_) for token in doc_student_100]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# TF-IDF + Cosine Similarity for objective-type questions\n",
    "def compute_tf_idf_similarity(answer, correct_answer):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([answer, correct_answer])\n",
    "    return cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "\n",
    "# Function to evaluate a single answer\n",
    "def evaluate_answer(student_answer, correct_answer, question_type):\n",
    "    # Preprocess text to remove stop words\n",
    "    student_answer_processed = preprocess_text(student_answer)\n",
    "    correct_answer_processed = preprocess_text(correct_answer)\n",
    "\n",
    "    if question_type == \"objective\":\n",
    "        score = compute_tf_idf_similarity(student_answer_processed, correct_answer_processed)\n",
    "    else:\n",
    "        # Use Sentence-BERT for semantic similarity\n",
    "        student_emb = sbert_model.encode(student_answer_processed, convert_to_tensor=True)\n",
    "        correct_emb = sbert_model.encode(correct_answer_processed, convert_to_tensor=True)\n",
    "        score = util.pytorch_cos_sim(student_emb, correct_emb).item()\n",
    "\n",
    "    # Apply a threshold to filter out low-similarity matches\n",
    "    if score < 0.5:  # Adjusted threshold to 0.2\n",
    "        score = 0.0\n",
    "    return round(score, 2)\n",
    "\n",
    "# Function to clean and organize the text\n",
    "def organize_text(text):\n",
    "    # Remove extra spaces, newlines, and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Ensure consistent formatting for subparts (e.g., 1(a)(i))\n",
    "    text = re.sub(r'(\\d+\\([a-z]\\)(?:\\([i-v]+\\))?)\\s*\\.?\\s*', r'\\1. ', text)\n",
    "\n",
    "    # Split into lines for further processing\n",
    "    lines = text.split('. ')\n",
    "    organized_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Ensure each line starts with a question or subpart identifier\n",
    "        if re.match(r'\\d+\\([a-z]\\)(?:\\([i-v]+\\))?', line):\n",
    "            organized_lines.append(line)\n",
    "        else:\n",
    "            # Append to the previous line if it doesn't start with an identifier\n",
    "            if organized_lines:\n",
    "                organized_lines[-1] += ' ' + line\n",
    "\n",
    "    # Join the lines back into a single string\n",
    "    organized_text = '. '.join(organized_lines)\n",
    "    return organized_text\n",
    "\n",
    "def simplify_subpart_key(key):\n",
    "    \"\"\"\n",
    "    Simplify subpart keys like 1(a)(i) to 1(a), but leave 1(a)(ii) unchanged.\n",
    "    \"\"\"\n",
    "    # Regex to match subpart keys like 1(a)(i) or 1(a)(ii)\n",
    "    pattern = re.compile(r'(\\d+\\([a-z]\\))(\\(i\\))')\n",
    "    match = pattern.search(key)\n",
    "    if match:\n",
    "        # If the key ends with (i), simplify it to the main part\n",
    "        return match.group(1)\n",
    "    return key  # Otherwise, return the key as is\n",
    "\n",
    "# Function to parse the answer text into questions and subparts\n",
    "def parse_answers(answer_text):\n",
    "    # Regex to detect question and subpart patterns (e.g., 1(a), 1(a)(i), etc.)\n",
    "    pattern = re.compile(r'(\\d+\\([a-z]\\)(?:\\([i-v]+\\))?)\\.?\\s*(.*?)(?=\\d+\\([a-z]\\)(?:\\([i-v]+\\))?|$)', re.DOTALL)\n",
    "    matches = pattern.findall(answer_text)\n",
    "    parsed_answers = {}\n",
    "    for match in matches:\n",
    "        key = match[0].strip()  # Question or subpart identifier (e.g., 1(a), 1(a)(i))\n",
    "        key = simplify_subpart_key(key)  # Simplify the key if necessary\n",
    "        content = match[1].strip()  # Content of the answer\n",
    "        parsed_answers[key] = content\n",
    "    return parsed_answers\n",
    "\n",
    "# Function to evaluate the entire answer sheet\n",
    "def evaluate_answer_sheet(teacher_answer_text, student_answer_text):\n",
    "    # Organize the text before parsing\n",
    "    teacher_answer_text = organize_text(teacher_answer_text)\n",
    "    student_answer_text = organize_text(student_answer_text)\n",
    "\n",
    "    # Parse teacher's and student's answers\n",
    "    teacher_answers = parse_answers(teacher_answer_text)\n",
    "    student_answers = parse_answers(student_answer_text)\n",
    "\n",
    "    # Debug: Print parsed answers\n",
    "    print(\"Parsed Teacher Answers:\")\n",
    "    for key, value in teacher_answers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nParsed Student Answers:\")\n",
    "    for key, value in student_answers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    total_score = 0.0\n",
    "    results = []\n",
    "    marks_dict = {}  # Dictionary to store marks for each subpart\n",
    "\n",
    "    # First, evaluate all subparts and store their marks\n",
    "    subpart_results = {}\n",
    "    for subpart_key in teacher_answers:\n",
    "        if subpart_key not in marks_dict:\n",
    "            marks_dict[subpart_key] = float(input(f\"Enter the maximum marks for {subpart_key}: \"))\n",
    "\n",
    "        student_answer = student_answers.get(subpart_key, \"\")\n",
    "        teacher_answer = teacher_answers.get(subpart_key, \"\")\n",
    "        similarity = evaluate_answer(student_answer, teacher_answer, \"descriptive\")\n",
    "        marks_awarded = round(similarity * marks_dict[subpart_key], 2)\n",
    "        subpart_results[subpart_key] = {\n",
    "            \"student_answer\": student_answer,\n",
    "            \"teacher_answer\": teacher_answer,\n",
    "            \"similarity\": similarity,\n",
    "            \"marks_awarded\": marks_awarded\n",
    "        }\n",
    "\n",
    "    # Now, group subparts under their respective main parts\n",
    "    main_parts = {}\n",
    "    for key in teacher_answers:\n",
    "        if re.match(r'\\d+\\([a-z]\\)$', key):  # Main part (e.g., 1(a))\n",
    "            main_parts[key] = []\n",
    "\n",
    "    for key in teacher_answers:\n",
    "        if re.match(r'\\d+\\([a-z]\\)(\\([i-v]+\\))?$', key):  # Subpart (e.g., 1(a)(i))\n",
    "            main_part_key = re.match(r'\\d+\\([a-z]\\)', key).group()\n",
    "            if main_part_key in main_parts:\n",
    "                main_parts[main_part_key].append(key)\n",
    "\n",
    "    # Aggregate the results by main part\n",
    "    for main_part_key, subpart_keys in main_parts.items():\n",
    "        question_score = 0.0\n",
    "        question_results = []\n",
    "\n",
    "        for subpart_key in subpart_keys:\n",
    "            question_score += subpart_results[subpart_key][\"marks_awarded\"]\n",
    "            question_results.append({\n",
    "                \"subpart\": subpart_key,\n",
    "                **subpart_results[subpart_key]\n",
    "            })\n",
    "\n",
    "        # Add the question results to the overall results\n",
    "        results.append({\n",
    "            \"question\": main_part_key,\n",
    "            \"question_score\": question_score,\n",
    "            \"subparts\": question_results\n",
    "        })\n",
    "        total_score += question_score\n",
    "\n",
    "    return {\"total_score\": total_score, \"results\": results}\n",
    "\n",
    "\n",
    "\n",
    "# Example teacher's and student's answers\n",
    "\n",
    "\n",
    "# Evaluate the answer sheet\n",
    "test_results = evaluate_answer_sheet(teacher_answer_text, student_answer_text)\n",
    "\n",
    "# Print the results\n",
    "for question in test_results[\"results\"]:\n",
    "    print(f\"\\nQuestion {question['question']}:\")\n",
    "    for subpart in question[\"subparts\"]:\n",
    "        print(f\"  Subpart {subpart['subpart']}:\")\n",
    "        print(f\"    Student Answer: {subpart['student_answer']}\")\n",
    "        print(f\"    Teacher Answer: {subpart['teacher_answer']}\")\n",
    "        print(f\"    Similarity: {subpart['similarity']}\")\n",
    "        print(f\"    Marks Awarded: {subpart['marks_awarded']}\")\n",
    "    print(f\"  Total Marks for Question {question['question']}: {question['question_score']}\")\n",
    "\n",
    "print(f\"\\nTotal Score: {test_results['total_score']}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess text by removing stop words\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Extract keyphrases from the teacher's answer\n",
    "def extract_keyphrases(text):\n",
    "    doc = nlp(text)\n",
    "    keyphrases = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.text.lower() not in stopwords.words('english'):\n",
    "            keyphrases.append(chunk.text)\n",
    "    return keyphrases\n",
    "\n",
    "# Fact-checking function\n",
    "def fact_check(student_answer, teacher_answer):\n",
    "    # Extract keyphrases from the teacher's answer\n",
    "    teacher_keyphrases = extract_keyphrases(teacher_answer)\n",
    "\n",
    "    # Check if the student's answer contains the keyphrases or their semantic equivalents\n",
    "    student_doc = nlp(student_answer)\n",
    "    for phrase in teacher_keyphrases:\n",
    "        phrase_doc = nlp(phrase)\n",
    "        # Check for exact match or semantic similarity\n",
    "        if not any(token.text.lower() == phrase.lower() for token in student_doc):\n",
    "            # If no exact match, check semantic similarity\n",
    "            student_emb = sbert_model.encode(student_answer, convert_to_tensor=True)\n",
    "            phrase_emb = sbert_model.encode(phrase, convert_to_tensor=True)\n",
    "            similarity = util.pytorch_cos_sim(student_emb, phrase_emb).item()\n",
    "            if similarity < 0.6:  # Threshold for semantic similarity\n",
    "                return False  # Factually incorrect\n",
    "    return True  # Factually correct\n",
    "\n",
    "# Function to evaluate a single answer\n",
    "def evaluate_answer(student_answer, teacher_answer, question_type):\n",
    "    student_answer_processed = preprocess_text(student_answer)\n",
    "    teacher_answer_processed = preprocess_text(teacher_answer)\n",
    "\n",
    "    # Perform fact-checking\n",
    "    if not fact_check(student_answer, teacher_answer):\n",
    "        return 0.0  # Factually incorrect answers get 0 marks\n",
    "\n",
    "    # Compute semantic similarity\n",
    "    if question_type == \"objective\":\n",
    "        semantic_score = compute_tf_idf_similarity(student_answer_processed, teacher_answer_processed)\n",
    "    else:\n",
    "        student_emb = sbert_model.encode(student_answer_processed, convert_to_tensor=True)\n",
    "        teacher_emb = sbert_model.encode(teacher_answer_processed, convert_to_tensor=True)\n",
    "        semantic_score = util.pytorch_cos_sim(student_emb, teacher_emb).item()\n",
    "\n",
    "    # Apply the scoring logic\n",
    "    if semantic_score <= 0.5:\n",
    "        score = 0.0  # 0 marks\n",
    "    elif 0.5 < semantic_score < 0.8:\n",
    "        score = 0.5  # Half marks\n",
    "    else:\n",
    "        score = 1.0  # Full marks\n",
    "\n",
    "    return round(score, 2)\n",
    "\n",
    "# Function to compute TF-IDF similarity\n",
    "def compute_tf_idf_similarity(answer, correct_answer):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([answer, correct_answer])\n",
    "    return cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "\n",
    "\n",
    "# Function to evaluate a single answer\n",
    "def evaluate_answer(student_answer, teacher_answer, question_type):\n",
    "    student_answer_processed = preprocess_text(student_answer)\n",
    "    teacher_answer_processed = preprocess_text(teacher_answer)\n",
    "\n",
    "    # Compute semantic similarity\n",
    "    if question_type == \"objective\":\n",
    "        semantic_score = compute_tf_idf_similarity(student_answer_processed, teacher_answer_processed)\n",
    "    else:\n",
    "        student_emb = sbert_model.encode(student_answer_processed, convert_to_tensor=True)\n",
    "        teacher_emb = sbert_model.encode(teacher_answer_processed, convert_to_tensor=True)\n",
    "        semantic_score = util.pytorch_cos_sim(student_emb, teacher_emb).item()\n",
    "\n",
    "    # Apply the scoring logic\n",
    "    if semantic_score >= 0.8:  # Full marks for highly similar answers\n",
    "        score = 1.0\n",
    "    elif 0.5 <= semantic_score < 0.8:  # Half marks for moderately similar answers\n",
    "        score = 0.5\n",
    "    else:  # 0 marks for low similarity\n",
    "        score = 0.0\n",
    "\n",
    "    return round(score, 1)  # Round to one decimal place\n",
    "\n",
    "# Function to evaluate the entire answer sheet\n",
    "def evaluate_answer_sheet(teacher_answer_text, student_answer_text):\n",
    "    teacher_answer_text = organize_text(teacher_answer_text)\n",
    "    student_answer_text = organize_text(student_answer_text)\n",
    "\n",
    "    teacher_answers = parse_answers(teacher_answer_text)\n",
    "    student_answers = parse_answers(student_answer_text)\n",
    "\n",
    "    print(\"Parsed Teacher Answers:\")\n",
    "    for key, value in teacher_answers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nParsed Student Answers:\")\n",
    "    for key, value in student_answers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    total_score = 0.0\n",
    "    results = []\n",
    "    marks_dict = {}\n",
    "\n",
    "    subpart_results = {}\n",
    "    for subpart_key in teacher_answers:\n",
    "        if subpart_key not in marks_dict:\n",
    "            marks_dict[subpart_key] = float(input(f\"Enter the maximum marks for {subpart_key}: \"))\n",
    "\n",
    "        student_answer = student_answers.get(subpart_key, \"\")\n",
    "        teacher_answer = teacher_answers.get(subpart_key, \"\")\n",
    "        similarity = evaluate_answer(student_answer, teacher_answer, \"descriptive\")\n",
    "        marks_awarded = round(similarity * marks_dict[subpart_key], 1)  # Round to one decimal place\n",
    "        subpart_results[subpart_key] = {\n",
    "            \"student_answer\": student_answer,\n",
    "            \"teacher_answer\": teacher_answer,\n",
    "            \"similarity\": similarity,\n",
    "            \"marks_awarded\": marks_awarded\n",
    "        }\n",
    "\n",
    "    main_parts = {}\n",
    "    for key in teacher_answers:\n",
    "        if re.match(r'\\d+\\([a-z]\\)$', key):\n",
    "            main_parts[key] = []\n",
    "\n",
    "    for key in teacher_answers:\n",
    "        if re.match(r'\\d+\\([a-z]\\)(\\([i-v]+\\))?$', key):\n",
    "            main_part_key = re.match(r'\\d+\\([a-z]\\)', key).group()\n",
    "            if main_part_key in main_parts:\n",
    "                main_parts[main_part_key].append(key)\n",
    "\n",
    "    for main_part_key, subpart_keys in main_parts.items():\n",
    "        question_score = 0.0\n",
    "        question_results = []\n",
    "\n",
    "        for subpart_key in subpart_keys:\n",
    "            question_score += subpart_results[subpart_key][\"marks_awarded\"]\n",
    "            question_results.append({\n",
    "                \"subpart\": subpart_key,\n",
    "                **subpart_results[subpart_key]\n",
    "            })\n",
    "\n",
    "        results.append({\n",
    "            \"question\": main_part_key,\n",
    "            \"question_score\": round(question_score, 1),  # Round to one decimal place\n",
    "            \"subparts\": question_results\n",
    "        })\n",
    "        total_score += question_score\n",
    "\n",
    "    return {\"total_score\": round(total_score, 1), \"results\": results}  # Round total score to one decimal place\n",
    "\n",
    "# Function to clean and organize the text\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def organize_text_with_gemini(text, api_key):\n",
    "    \"\"\"\n",
    "    Uses Google's Gemini to structure unorganized extracted text into a properly formatted answer sheet.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The unorganized extracted text from answer sheets\n",
    "        api_key (str): Your Google AI Studio API key\n",
    "        \n",
    "    Returns:\n",
    "        str: Well-structured answer sheet text\n",
    "    \"\"\"\n",
    "    genai.configure(api_key=api_key)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in structuring unorganized answer sheets for grading. \n",
    "    The following text was extracted from an answer sheet but is poorly organized:\n",
    "    \n",
    "    {text}\n",
    "    \n",
    "    Please:\n",
    "    1. Identify all questions and sub-questions (like 1(a), 1(a)(i), etc.)\n",
    "    2. Structure them in a clear, readable format\n",
    "    3. Maintain proper numbering and indentation\n",
    "    4. Preserve all original answer content\n",
    "    5. Output only the structured text with no additional commentary\n",
    "    \n",
    "    Structured version:\n",
    "    \"\"\"\n",
    "    \n",
    "    model = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Process the streaming response\n",
    "    structured_text = \"\"\n",
    "    for chunk in response:\n",
    "        structured_text += chunk.text\n",
    "    \n",
    "    return structured_text\n",
    "\n",
    "# Example usage:\n",
    "API_KEY = \"your_api_key_here\"  # Replace with your actual API key\n",
    "unorganized_text = \"1a i AnswerA 1a ii AnswerB 1b AnswerC...\"\n",
    "\n",
    "structured_result = organize_text_with_gemini(unorganized_text, API_KEY)\n",
    "display(Markdown(f\"**Structured Answer Sheet:**\\n\\n{structured_result}\"))\n",
    "\n",
    "def simplify_subpart_key(key):\n",
    "    pattern = re.compile(r'(\\d+\\([a-z]\\))(\\(i\\))')\n",
    "    match = pattern.search(key)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return key\n",
    "\n",
    "# Function to parse the answer text into questions and subparts\n",
    "def parse_answers(answer_text):\n",
    "    pattern = re.compile(r'(\\d+\\([a-z]\\)(?:\\([i-v]+\\))?)\\s*[\\.\\-]?\\s*(.+?)(?=\\n\\d+\\([a-z]\\)|$)', re.DOTALL)\n",
    "    matches = pattern.findall(answer_text)\n",
    "\n",
    "    parsed_answers = {}\n",
    "    for match in matches:\n",
    "        key = match[0].strip()\n",
    "        content = match[1].strip()\n",
    "        parsed_answers[key] = content\n",
    "\n",
    "    print(\"\\nðŸ” Parsed Answers:\")\n",
    "    for key, value in parsed_answers.items():\n",
    "        print(f\"{key}: {value[:50]}...\")  # Print first 50 chars of each answer\n",
    "\n",
    "    return parsed_answers\n",
    "\n",
    "\n",
    "# Function to evaluate the entire answer sheet\n",
    "def evaluate_answer_sheet(teacher_answer_text, student_answer_text):\n",
    "    teacher_answer_text = organize_text(teacher_answer_text)\n",
    "    student_answer_text = organize_text(student_answer_text)\n",
    "\n",
    "    teacher_answers = parse_answers(teacher_answer_text)\n",
    "    student_answers = parse_answers(student_answer_text)\n",
    "\n",
    "    print(\"Parsed Teacher Answers:\")\n",
    "    for key, value in teacher_answers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nParsed Student Answers:\")\n",
    "    for key, value in student_answers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    total_score = 0.0\n",
    "    results = []\n",
    "    marks_dict = {}\n",
    "\n",
    "    subpart_results = {}\n",
    "    for subpart_key in teacher_answers:\n",
    "        if subpart_key not in marks_dict:\n",
    "            marks_dict[subpart_key] = float(input(f\"Enter the maximum marks for {subpart_key}: \"))\n",
    "\n",
    "        student_answer = student_answers.get(subpart_key, \"\")\n",
    "        teacher_answer = teacher_answers.get(subpart_key, \"\")\n",
    "        similarity = evaluate_answer(student_answer, teacher_answer, \"descriptive\")\n",
    "        marks_awarded = round(similarity * marks_dict[subpart_key], 2)\n",
    "        subpart_results[subpart_key] = {\n",
    "            \"student_answer\": student_answer,\n",
    "            \"teacher_answer\": teacher_answer,\n",
    "            \"similarity\": similarity,\n",
    "            \"marks_awarded\": marks_awarded\n",
    "        }\n",
    "\n",
    "    main_parts = {}\n",
    "    for key in teacher_answers:\n",
    "        if re.match(r'\\d+\\([a-z]\\)$', key):\n",
    "            main_parts[key] = []\n",
    "\n",
    "    for key in teacher_answers:\n",
    "        if re.match(r'\\d+\\([a-z]\\)(\\([i-v]+\\))?$', key):\n",
    "            main_part_key = re.match(r'\\d+\\([a-z]\\)', key).group()\n",
    "            if main_part_key in main_parts:\n",
    "                main_parts[main_part_key].append(key)\n",
    "\n",
    "    for main_part_key, subpart_keys in main_parts.items():\n",
    "        question_score = 0.0\n",
    "        question_results = []\n",
    "\n",
    "        for subpart_key in subpart_keys:\n",
    "            question_score += subpart_results[subpart_key][\"marks_awarded\"]\n",
    "            question_results.append({\n",
    "                \"subpart\": subpart_key,\n",
    "                **subpart_results[subpart_key]\n",
    "            })\n",
    "\n",
    "        results.append({\n",
    "            \"question\": main_part_key,\n",
    "            \"question_score\": question_score,\n",
    "            \"subparts\": question_results\n",
    "        })\n",
    "        total_score += question_score\n",
    "\n",
    "    return {\"total_score\": total_score, \"results\": results}\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    teacher_answer = input(\"Enter the teacher's paragraph: \")\n",
    "    student_answer = input(\"Enter the student's paragraph: \")\n",
    "\n",
    "    # # Perform sentiment analysis\n",
    "    # sentiment_result = sentiment_analysis(teacher_answer, student_answer)\n",
    "    # print(\"Sentiment Analysis Result:\")\n",
    "    # print(sentiment_result)\n",
    "\n",
    "    # Evaluate the answer sheet\n",
    "    test_results = evaluate_answer_sheet(teacher_answer, student_answer)\n",
    "    print(\"\\nAnswer Sheet Evaluation Results:\")\n",
    "    for question in test_results[\"results\"]:\n",
    "        print(f\"\\nQuestion {question['question']}:\")\n",
    "        for subpart in question[\"subparts\"]:\n",
    "            print(f\"  Subpart {subpart['subpart']}:\")\n",
    "            print(f\"    Student Answer: {subpart['student_answer']}\")\n",
    "            print(f\"    Teacher Answer: {subpart['teacher_answer']}\")\n",
    "            print(f\"    Similarity: {subpart['similarity']}\")\n",
    "            print(f\"    Marks Awarded: {subpart['marks_awarded']}\")\n",
    "        print(f\"  Total Marks for Question {question['question']}: {question['question_score']}\")\n",
    "\n",
    "    print(f\"\\nTotal Score: {test_results['total_score']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
